# Reflections

## Jakub Cerovsky

Figuring out what XR is and how it covers AR, VR, and other systems working with different realities was a challenge I decided to take on during the last semester. I thought that while being a student at VIA, it would be easy to gain access to hardware and knowledge I would have a hard time acquiring on my own. The fact that this assumption was only half-correct is something I figured out only after the semester started.

As a person with zero experience in game development and working with Unity, the realisation that this subject will teach me none of that and only introduce project ideas I have to work on during my bachelor's semester was an unwelcome wake-up call. Starting with such a skill gap between me and other team members and me made me almost utterly useless in the initial lab days of AR development. I spent most of the time looking over Alex's shoulder and asking him for an explanation and reasoning behind his actions. After getting a little insight and individually reproducing functionality of the system in my free time, I ended up being able to add book objects and prepare the NavMesh (bake walkable paths in the 3D scan used by guidance line) for the Library Navigation System. This sounds like a small achievement, but to me it was a great feeling being able to actually contribute in my own way, which also motivated me to try even harder in the next part of the semester.

During the development of the VR project, I got my hands on the headsets and was able to learn how to turn them on (it took me minutes to figure out) and play with them. What I found interesting is the development improvements and considerations behind individual versions; all the thoughts behind these decisions, such as Lens Distortion or Immersive Audio, and how they affect the experience the user feels. These things I have never even considered before, all I thought was - "You just get a monitor attached to your head" - but there is so much more to it. This definitely got me interested in finding out more about this technology, even after studying.

Furthermore, working on the VR project with at least the little knowledge I gained through the AR project made such a difference. I felt confident in taking on the whole part of the shapes puzzle attached to the bomb and developed it with considerations for the immersion of the user. Adding physics in the form of kinematic and dynamic objects to simulate the collision behaviour was the first step to make the user feel like they are actually working with real objects. I followed the [VR Development](https://learn.unity.com/pathway/vr-development?version=6.2) guide shared by Unity, as well as their [XR Interaction Toolkit](https://github.com/Unity-Technologies/XR-Interaction-Toolkit-Examples/blob/main/Documentation/index.md), trying to find features that would work well with my assigned puzzle. What I found really exciting was the Gaze Assistance that tracks eye movement; however, this feature is only available in the newer versions of headsets that we did not get to work with. I managed to implement the Grabbable logic and Socketting system of the puzzle, and later on played around with the option to "hide" the controller. At the same time, the object is grabbed, and also to not make it possible to grab an object from being grabbed from a far distance. These features would definitely make the user immersed, as it would require them to reach for things, grab them at a real distance, and interact with them directly from their hand instead of from the other side of the room. Unfortunately, this made the solving of the puzzle more difficult than fun, and we decided not to include it.

Overall, the impression I got from this subject was somewhat mixed; it fulfilled my wishes to learn about the given technologies and work with hardware at no financial expense. However, the fact that I had to learn everything myself while working on my bachelor's really took a lot of my mental health, and I would end up knowing the same by going through the guides and tutorials after my education was concluded. This was caused by my lack of experience with Unity, and would likely be resolved if I were to take on the Game Development course during the last semester. Even though it was difficult, I am extremely proud of our VR project and my contribution there, and if time permitted, I would improve the AR project, which did not end up with the best results. I doubt that this was my last interaction with the XR development world, and I believe that the knowledge base I currently have will prove useful in the upcoming adventures.

## Alexandro Bolfa

Before this course, I had only used Unity for regular game development in GMD and had never touched AR or VR, so both projects felt a bit like being thrown into the deep end. In the AR project, I took over the “frontend” side, so I built most of the app’s UI flow. When the user opens the VIA library navigation app, they first see a welcome panel. Cosmin made the initial popup, and from there I added the button to hide it and built the rest of the interface on top.

After the welcome screen, the user sees a search button that opens an input field for the ISBN. Behind that, I made a small BookDatabase and the logic that checks whether the book exists in our system.

Once a book is found, the “XR” part of my work starts. I imported an asset for drawing a guidance line and adapted it so it wasn’t a fixed path but a dynamic one generated using the NavMesh baked from our 3D scan of the VIA library. The line updates as the user moves, follows walkable areas, and disappears if they cancel navigation. Getting this to work made many things from the course suddenly click: tracking noise, drift, imperfect scans and occlusion, all the things that complicate what seems like a simple pathfinding feature.

The biggest lesson I learned was that XR simulation in Unity is not reality. In the editor, everything looked perfect. On the phone, things broke in ways we didn’t expect. We had to test constantly, rebuild over and over, and accept that AR only truly behaves on a real device with a real camera and real tracking.

For the VR project, my work became much more physical and hands-on. I took ownership of the electrical box puzzle. First, I edited the model in Blender, removed an unnecessary lever, patched the mesh, separated the lid from the base, and positioned four screws around the corners to make holes in the lid's mesh. That cleanup was necessary so user wouldn't thik he can interact with the lever.

In Unity, I combined a drill body with two different tips (flat and cross) to create realistic electric screwdrivers. The screws on the box match those tips, so the user actually has to pick the right tool for each screw. I set up the logic so that when the spinning drill tip touches a screw and the user presses the trigger, the screw gradually backs out. Once all screws are removed, the lid becomes free and it falls on the ground, revealing the content of the box.

Inside the box, I added two metal parts connected with four wires. Only one wire is the correct one to cut; the other three cause an “explosion.” I set up the pliers interaction so that when both jaws close on a wire, it gets marked as cut and reports back to the bomb defuser manager. This part was super satisfying because it mixed together modelling, physics, colliders, and scripting into one complete interaction.

Looking at both projects, I can clearly see how my understanding of XR changed. In AR, most of the challenge was combining digital logic with the real world, where tracking and scanning are never perfect. In VR, I had full control over the environment, but then the challenge shifted to making interactions believable: the way screws move, the way tools respond, the way cutting a wire triggers a result.

Both projects made the theory from the course feel real. I wasn’t just reading about SLAM, NavMesh, 6DOF controllers, or XR interaction patterns, I was actually building things that depended on them. I also became much more aware of hardware limits, especially in VR, where standalone headsets can’t handle heavy meshes or expensive physics.

Overall, I’ve gone from “I know a bit of Unity” to someone who can design and justify an AR flow and a VR interaction system. I’m still learning, but I now understand how to approach XR problems: think about the user’s context, know the limits of the hardware, design around them; and always test on the real device, because nothing behaves the same in the editor.
