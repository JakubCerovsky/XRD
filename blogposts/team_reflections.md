# Reflections

## Jakub Cerovsky

Figuring out what XR is and how it covers AR, VR, and other systems working with different realities was a challenge I decided to take on during the last semester. I thought that while being a student at VIA, it would be easy to gain access to hardware and knowledge I would have a hard time acquiring on my own. The fact that this assumption was only half-correct is something I figured out only after the semester started.

As a person with zero experience in game development and working with Unity, the realisation that this subject will teach me none of that and only introduce project ideas I have to work on during my bachelor's semester was an unwelcome wake-up call. Starting with such a skill gap between me and other team members and me made me almost utterly useless in the initial lab days of AR development. I spent most of the time looking over Alex's shoulder and asking him for an explanation and reasoning behind his actions. After getting a little insight and individually reproducing functionality of the system in my free time, I ended up being able to add book objects and prepare the NavMesh (bake walkable paths in the 3D scan used by guidance line) for the Library Navigation System. This sounds like a small achievement, but to me it was a great feeling being able to actually contribute in my own way, which also motivated me to try even harder in the next part of the semester.

During the development of the VR project, I got my hands on the headsets and was able to learn how to turn them on (it took me minutes to figure out) and play with them. What I found interesting is the development improvements and considerations behind individual versions; all the thoughts behind these decisions, such as Lens Distortion or Immersive Audio, and how they affect the experience the user feels. These things I have never even considered before, all I thought was - "You just get a monitor attached to your head" - but there is so much more to it. This definitely got me interested in finding out more about this technology, even after studying.

Furthermore, working on the VR project with at least the little knowledge I gained through the AR project made such a difference. I felt confident in taking on the whole part of the shapes puzzle attached to the bomb and developed it with considerations for the immersion of the user. Adding physics in the form of kinematic and dynamic objects to simulate the collision behaviour was the first step to make the user feel like they are actually working with real objects. I followed the [VR Development](https://learn.unity.com/pathway/vr-development?version=6.2) guide shared by Unity, as well as their [XR Interaction Toolkit](https://github.com/Unity-Technologies/XR-Interaction-Toolkit-Examples/blob/main/Documentation/index.md), trying to find features that would work well with my assigned puzzle. What I found really exciting was the Gaze Assistance that tracks eye movement; however, this feature is only available in the newer versions of headsets that we did not get to work with. I managed to implement the Grabbable logic and Socketting system of the puzzle, and later on played around with the option to "hide" the controller. At the same time, the object is grabbed, and also to not make it possible to grab an object from being grabbed from a far distance. These features would definitely make the user immersed, as it would require them to reach for things, grab them at a real distance, and interact with them directly from their hand instead of from the other side of the room. Unfortunately, this made the solving of the puzzle more difficult than fun, and we decided not to include it.

Overall, the impression I got from this subject was somewhat mixed; it fulfilled my wishes to learn about the given technologies and work with hardware at no financial expense. However, the fact that I had to learn everything myself while working on my bachelor's really took a lot of my mental health, and I would end up knowing the same by going through the guides and tutorials after my education was concluded. This was caused by my lack of experience with Unity, and would likely be resolved if I were to take on the Game Development course during the last semester. Even though it was difficult, I am extremely proud of our VR project and my contribution there, and if time permitted, I would improve the AR project, which did not end up with the best results. I doubt that this was my last interaction with the XR development world, and I believe that the knowledge base I currently have will prove useful in the upcoming adventures.

## Marcus Mitelea

Before this course, the only thing I made that would count as XR was a instagram filter for privacy. I was using Facebook's tools for that. And my last serious interaction with Unity was years ago (around the Unity 4 era).

Both AR and VR projects started with a lot of “re-learning Unity”, but also learning an entirely new set of constraints: tracking quality in AR and physical, embodied interaction design in VR.

In the AR library navigation project, my contributions were mostly about making the app robust and “usable” in real conditions. I worked on the library environment setup by bringing in the NavMesh-based environment from my scan, ensuring the scene and prefabs were organized so navigation logic had something reliable to run on. On the input side, I implemented ISBN input validation directly in the UI controller, including basic formatting checks and feedback when the ISBN is not valid. I also explored camera-based ISBN scanning by integrating a barcode scanner flow and implementing the scripts that connect the scanner output to the ISBN pipeline. Unfortunately, I was constrained on time, so I ended up scrapping that idea.

Early on, I also experimented with pre-built indoor localization SDKs (MultiSet VPS and Immersal) to see if we could get more robust tracking. In practice, the tradeoffs (SDK size/overhead and their logo plastered on our project) pushed us back toward building on top of the core AR Foundation workflow so we could control and debug the full pipeline. While personally I would still have gone for cloud anchors, since the demo shown in class was image tracking, we just went with that.

A repeated lesson across the AR lab days was that XR Simulation is useful for iteration, but it’s not reality: things that looked stable in-editor often surfaced different problems on-device, so we had to get used to building frequently and validating changes in the actual library setup. This also took an annoyingly long time on my and Jakub's machine.

For the VR bomb-defusal project, I came up with the puzzles and owned the keypad + timer mechanics. I implemented the keypad interaction with dedicated scripts for individual button presses and the passcode lock logic, wired it to TextMeshPro feedback, and packaged it into prefabs/scenes, so it was easy to integrate. I then added the countdown timer system (including a floating countdown display and timer assets/prefabs, which we scrapped later on) and integrated it into the overall defusal flow. The final integration tied the keypad lock into the game manager with wrong-attempt tracking, and the timer only starts once the tape is cut, which made the sequence feel intentional instead of “everything happens at once.”

One surprisingly “real” part of that work was the timer UI itself: instead of relying on a perfect 3D clock asset, I ended up using a simple 2D background plus TextMeshPro to make the timer readable in-world, and then focused on wiring it cleanly into the bomb state. Partly this is because I refused to pay for an asset, and a quick image plus the correct font did the trick just fine

On the process side, these lab days also made the cost of iteration feel very real: build times, device testing, and even platform quirks (like Unity behaving differently across machines) affected how we planned and integrated features. You will never catch me opening Unity on an office laptop again. Plus, Unity on my Linux laptop was buggy. We also had some git conflicts that wiped the scene we were working on in the AR project, but we did learn our lessons and for VR, we had different scenes.

The thing that surprised me the most is how much of the manual labor from the old days is automated now in unity. Back when I was playing around with VR/AR, you needed to do almost everything by hand. Now physics, controllers, space tracking, interaction between objects etc is just a few clicks away.

Overall, I’m leaving the course with a much better understanding of how XR projects live or die on integration details: clean scene setup, predictable state transitions, and constant testing. Credit where credit is due. I am thankful to Cosmin for allowing me to test on his machine often when I needed revisions and for integrating sounds for my parts while we were on call. Jakub showed me once again how important it is being organised, as he made a flowchart, and took care of checklists that proved very useful in alligning our work together. And Alex was just a nice guy and did his stuff, which I am also thankful for.

## Alexandro Bolfa

Before this course, I had only used Unity for regular game development in GMD and had never touched AR or VR, so both projects felt a bit like being thrown into the deep end. In the AR project, I took over the “frontend” side, so I built most of the app’s UI flow. When the user opens the VIA library navigation app, they first see a welcome panel. Cosmin made the initial popup, and from there I added the button to hide it and built the rest of the interface on top.

After the welcome screen, the user sees a search button that opens an input field for the ISBN. Behind that, I made a small BookDatabase and the logic that checks whether the book exists in our system.

Once a book is found, the “XR” part of my work starts. I imported an asset for drawing a guidance line and adapted it so it wasn’t a fixed path but a dynamic one generated using the NavMesh baked from our 3D scan of the VIA library. The line updates as the user moves, follows walkable areas, and disappears if they cancel navigation. Getting this to work made many things from the course suddenly click: tracking noise, drift, imperfect scans and occlusion, all the things that complicate what seems like a simple pathfinding feature.

The biggest lesson I learned was that XR simulation in Unity is not reality. In the editor, everything looked perfect. On the phone, things broke in ways we didn’t expect. We had to test constantly, rebuild over and over, and accept that AR only truly behaves on a real device with a real camera and real tracking.

For the VR project, my work became much more physical and hands-on. I took ownership of the electrical box puzzle. First, I edited the model in Blender, removed an unnecessary lever, patched the mesh, separated the lid from the base, and positioned four screws around the corners to make holes in the lid's mesh. That cleanup was necessary so user wouldn't thik he can interact with the lever.

In Unity, I combined a drill body with two different tips (flat and cross) to create realistic electric screwdrivers. The screws on the box match those tips, so the user actually has to pick the right tool for each screw. I set up the logic so that when the spinning drill tip touches a screw and the user presses the trigger, the screw gradually backs out. Once all screws are removed, the lid becomes free and it falls on the ground, revealing the content of the box.

Inside the box, I added two metal parts connected with four wires. Only one wire is the correct one to cut; the other three cause an “explosion.” I set up the pliers interaction so that when both jaws close on a wire, it gets marked as cut and reports back to the bomb defuser manager. This part was super satisfying because it mixed together modelling, physics, colliders, and scripting into one complete interaction.

Looking at both projects, I can clearly see how my understanding of XR changed. In AR, most of the challenge was combining digital logic with the real world, where tracking and scanning are never perfect. In VR, I had full control over the environment, but then the challenge shifted to making interactions believable: the way screws move, the way tools respond, the way cutting a wire triggers a result.

Both projects made the theory from the course feel real. I wasn’t just reading about SLAM, NavMesh, 6DOF controllers, or XR interaction patterns, I was actually building things that depended on them. I also became much more aware of hardware limits, especially in VR, where standalone headsets can’t handle heavy meshes or expensive physics.

Overall, I’ve gone from “I know a bit of Unity” to someone who can design and justify an AR flow and a VR interaction system. I’m still learning, but I now understand how to approach XR problems: think about the user’s context, know the limits of the hardware, design around them; and always test on the real device, because nothing behaves the same in the editor.
